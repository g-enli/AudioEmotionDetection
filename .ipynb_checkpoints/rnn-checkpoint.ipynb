{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ses Tabanlı Duygu Tanıma (Speech Emotion Recognition - SER)\n",
    "Bu notebook, **CREMA-D** ve **EMO-DB** veri setlerini kullanarak ses dosyalarından duygu tanıma işlemi gerçekleştirir. RNN (LSTM) modeli kullanılır. Adımlar:\n",
    "1.  **Veri Artırma (Data Augmentation):** Ses verilerini zenginleştirmek için gürültü ekleme, hızlandırma, perde kaydırma (pitch shift) gibi teknikler kullanılır.(https://www.researchgate.net/publication/372809586_Speech_emotion_recognition_with_light_gradient_boosting_decision_trees_machine)\n",
    "2. **Öznitelik Çıkarma**: MFCC zaman serileri.\n",
    "3. **Veri Ön İşleme**: Eğitim/doğrulama/test bölmesi, StandardScaler ile ölçeklendirme.\n",
    "4. **Sınıf Dengesizliği (Opsiyonel)**: SMOTE ile dengeleme.\n",
    "5. **Modelleme**: LSTM tabanlı RNN modeli eğitilir ve kaydedilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gerekli Kütüphaneler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Veri Yükleme ve Öznitelik Çıkarma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_path, emotion_map, dataset_name, max_per_class=50, fixed_length=32000, use_augmentation=True):\n",
    "    \"\"\"Veri setini yükler, artırır ve MFCC özniteliklerini zaman serisi olarak çıkarır.\"\"\"\n",
    "    features, labels = [], []\n",
    "    sample_rate = 22050 if dataset_name == 'CREMA-D' else 16000\n",
    "    print(f\"{dataset_name} veri seti yükleniyor... (Sample Rate: {sample_rate})\")\n",
    "    \n",
    "    for emotion_code, emotion_name in emotion_map.items():\n",
    "        path_pattern = os.path.join(dataset_path, f'*{emotion_code}*.wav')\n",
    "        files = glob.glob(path_pattern)[:max_per_class]\n",
    "        if not files:\n",
    "            print(f\"Uyarı: {emotion_code} ({emotion_name}) sınıfında dosya bulunamadı.\")\n",
    "            continue\n",
    "        print(f\"{emotion_name} sınıfı: {len(files)} dosya bulundu.\")\n",
    "        for file_name in files:\n",
    "            try:\n",
    "                y, sr = librosa.load(file_name, sr=sample_rate, res_type='kaiser_fast')\n",
    "                if len(y) == 0:\n",
    "                    print(f\"Uyarı: {file_name} boş veya hatalı.\")\n",
    "                    continue\n",
    "                # Ses uzunluğunu sabitle\n",
    "                if len(y) > fixed_length:\n",
    "                    y = y[:fixed_length]\n",
    "                else:\n",
    "                    y = np.pad(y, (0, max(0, fixed_length - len(y))), mode='constant')\n",
    "                \n",
    "                # Veri artırma\n",
    "                audios = augment_audio(y, sr, fixed_length) if use_augmentation else [y]\n",
    "                for audio in audios:\n",
    "                    # MFCC özniteliklerini çıkar (zaman serisi)\n",
    "                    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "                    features.append(mfcc.T)  # (time_steps, n_mfcc)\n",
    "                    labels.append(emotion_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Hata: {file_name} işlenemedi. Hata: {str(e)}\")\n",
    "    \n",
    "    if not features:\n",
    "        print(f\"Hata: {dataset_name} için hiç öznitelik çıkarılamadı.\")\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    # Zaman serilerini aynı uzunluğa getir\n",
    "    max_time_steps = max(f.shape[0] for f in features)\n",
    "    features_padded = np.array([np.pad(f, ((0, max_time_steps - f.shape[0]), (0, 0)), mode='constant') for f in features])\n",
    "    labels = np.array(labels)\n",
    "    print(f\"{dataset_name}: Toplam {len(features_padded)} örnek, Zaman adımı: {max_time_steps}\")\n",
    "    return features_padded, labels\n",
    "\n",
    "def augment_audio(y, sr, fixed_length=32000):\n",
    "    \"\"\"Ses verisini artırmak için gürültü, hız değiştirme ve perde kaydırma uygular.\"\"\"\n",
    "    augmented = []\n",
    "    y = y / np.max(np.abs(y)) if np.max(np.abs(y)) != 0 else y\n",
    "    if len(y) > fixed_length:\n",
    "        y = y[:fixed_length]\n",
    "    else:\n",
    "        y = np.pad(y, (0, fixed_length - len(y)), mode='constant')\n",
    "    augmented.append(y)\n",
    "    \n",
    "    noise = y + 0.005 * np.random.randn(len(y))\n",
    "    augmented.append(noise)\n",
    "    \n",
    "    try:\n",
    "        speed = librosa.effects.time_stretch(y, rate=1.1)\n",
    "        if len(speed) > fixed_length:\n",
    "            speed = speed[:fixed_length]\n",
    "        else:\n",
    "            speed = np.pad(speed, (0, fixed_length - len(speed)), mode='constant')\n",
    "        augmented.append(speed)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        pitch = librosa.effects.pitch_shift(y, sr=sr, n_steps=np.random.uniform(-1, 1))\n",
    "        if len(pitch) > fixed_length:\n",
    "            pitch = pitch[:fixed_length]\n",
    "        else:\n",
    "            pitch = np.pad(pitch, (0, fixed_length - len(pitch)), mode='constant')\n",
    "        augmented.append(pitch)\n",
    "    except:\n",
    "        pass\n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RNN Modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size=13, hidden_size=64, num_layers=2, num_classes=6):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Veri Seti Sınıfı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Eğitim Fonksiyonu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, train_loader, val_loader, num_epochs=20, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}')\n",
    "\n",
    "        model.eval()\n",
    "        val_preds, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        print(f'Validation Accuracy: {val_acc:.4f}')\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f'rnn_model_{dataset_name}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Veri Seti Yolları ve Duygu Haritaları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = './'\n",
    "CREMA_D_PATH = os.path.join(BASE_PATH, 'datasets/CREMA-D')\n",
    "EMODB_PATH = os.path.join(BASE_PATH, 'datasets/EMO-DB')\n",
    "\n",
    "crema_emotion_map = {\n",
    "    'HAP': 'Happy',\n",
    "    'SAD': 'Sad',\n",
    "    'ANG': 'Angry',\n",
    "    'FEA': 'Fear',\n",
    "    'DIS': 'Disgust',\n",
    "    'NEU': 'Neutral'\n",
    "}\n",
    "\n",
    "emodb_emotion_map = {\n",
    "    'angry': 'Angry',\n",
    "    'happy': 'Happy',\n",
    "    'disgusted': 'Disgust',\n",
    "    'fearful': 'Fear',\n",
    "    'surprised': 'Surprised',\n",
    "    'sad': 'Sad',\n",
    "    'neutral': 'Neutral'\n",
    "}\n",
    "\n",
    "dataset_name = 'CREMA-D'\n",
    "dataset_path = CREMA_D_PATH if dataset_name == 'CREMA-D' else EMODB_PATH\n",
    "emotion_map = crema_emotion_map if dataset_name == 'CREMA-D' else emodb_emotion_map\n",
    "\n",
    "features, labels = load_data(dataset_path, emotion_map, dataset_name, max_per_class=50, use_augmentation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Veri Ön İşleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etiketleri sayısal formata çevir\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels)\n",
    "joblib.dump(le, f'label_encoder_{dataset_name}.pkl')\n",
    "\n",
    "# Veri setini böl\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, labels_encoded, test_size=0.3, random_state=42, stratify=labels_encoded)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Ölçeklendirme\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1)).reshape(X_train.shape)\n",
    "X_val_scaled = scaler.transform(X_val.reshape(X_val.shape[0], -1)).reshape(X_val.shape)\n",
    "X_test_scaled = scaler.transform(X_test.reshape(X_test.shape[0], -1)).reshape(X_test.shape)\n",
    "joblib.dump(scaler, f'scaler_{dataset_name}.pkl')\n",
    "\n",
    "# SMOTE (opsiyonel)\n",
    "use_smote = False\n",
    "if use_smote:\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_reshaped = X_train_scaled.reshape(X_train_scaled.shape[0], -1)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_reshaped, y_train)\n",
    "    X_train_balanced = X_train_balanced.reshape(-1, X_train_scaled.shape[1], X_train_scaled.shape[2])\n",
    "    print(f\"{dataset_name} - SMOTE sonrası eğitim seti boyutu: {X_train_balanced.shape}\")\n",
    "else:\n",
    "    X_train_balanced, y_train_balanced = X_train_scaled, y_train\n",
    "\n",
    "# Test verilerini kaydet\n",
    "np.save(f'X_test_{dataset_name}.npy', X_test_scaled)\n",
    "np.save(f'y_test_{dataset_name}.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RNN için Veri Hazırlığı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AudioDataset(X_train_balanced, y_train_balanced)\n",
    "val_dataset = AudioDataset(X_val_scaled, y_val)\n",
    "test_dataset = AudioDataset(X_test_scaled, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. RNN Modelini Eğitme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(labels_encoded))\n",
    "model = RNNModel(input_size=X_train_scaled.shape[2], hidden_size=64, num_layers=2, num_classes=num_classes)\n",
    "train_rnn(model, train_loader, val_loader, num_epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
